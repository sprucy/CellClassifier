Verbosity: 2 (Standard Logging)
=================== System Info ===================
AutoGluon Version:  1.1.1
Python Version:     3.11.9
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.22631
CPU Count:          8
Memory Avail:       3.44 GB / 15.73 GB (21.9%)
Disk Space Avail:   137.15 GB / 312.00 GB (44.0%)
===================================================
Presets specified: ['medium_quality']
Beginning AutoGluon training ... Time limit = 3600s
AutoGluon will save models to "models\Teacher\medium_quality-20241120-233321"
Train Data Rows:    108
Train Data Columns: 60
Label Column:       label
AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).
	3 unique label values:  ['Exc L2-3 LINC00507 FREM3 superficial', 'Exc L2-4 LINC00507 GLP2R', 'Exc L2 LAMP5 LTK']
	If 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])
Problem Type:       multiclass
Preprocessing data ...
Train Data Class Count: 3
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    3502.28 MB
	Train Data (Original)  Memory Usage: 0.05 MB (0.0% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Unused Original Features (Count: 1): ['Ends total']
		These features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.
		Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.
		These features do not need to be present at inference time.
		('float', []) : 1 | ['Ends total']
	Types of features in original data (raw dtype, special dtypes):
		('float', []) : 59 | ['input_resistance', 'sag', 'VmatSag', 'vmbaseM', 'tau', ...]
	Types of features in processed data (raw dtype, special dtypes):
		('float', []) : 59 | ['input_resistance', 'sag', 'VmatSag', 'vmbaseM', 'tau', ...]
	0.1s = Fit runtime
	59 features in original data used to generate 59 features in processed data.
	Train Data (Processed) Memory Usage: 0.05 MB (0.0% of available memory)
Data preprocessing and feature engineering runtime = 0.16s ...
AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'
	To change this, specify the eval_metric parameter of Predictor()
Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 86, Val Rows: 22
User-specified model hyperparameters to be fit:
{
	'NN_TORCH': {},
	'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],
	'CAT': {},
	'XGB': {},
	'FASTAI': {},
	'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
	'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
	'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],
}
Fitting 13 L1 models ...
Fitting model: KNeighborsUnif ... Training model for up to 3599.84s of the 3599.83s of remaining time.
	0.6364	 = Validation score   (accuracy)
	2.62s	 = Training   runtime
	0.65s	 = Validation runtime
Fitting model: KNeighborsDist ... Training model for up to 3596.57s of the 3596.57s of remaining time.
	0.7727	 = Validation score   (accuracy)
	0.0s	 = Training   runtime
	0.02s	 = Validation runtime
Fitting model: NeuralNetFastAI ... Training model for up to 3596.56s of the 3596.55s of remaining time.
No improvement since epoch 5: early stopping
	0.9091	 = Validation score   (accuracy)
	1.37s	 = Training   runtime
	0.01s	 = Validation runtime
Fitting model: LightGBMXT ... Training model for up to 3595.15s of the 3595.15s of remaining time.
	0.9091	 = Validation score   (accuracy)
	0.26s	 = Training   runtime
	0.0s	 = Validation runtime
Fitting model: LightGBM ... Training model for up to 3594.89s of the 3594.89s of remaining time.
	0.8182	 = Validation score   (accuracy)
	0.19s	 = Training   runtime
	0.0s	 = Validation runtime
Fitting model: RandomForestGini ... Training model for up to 3594.69s of the 3594.68s of remaining time.
	0.9545	 = Validation score   (accuracy)
	0.49s	 = Training   runtime
	0.04s	 = Validation runtime
Fitting model: RandomForestEntr ... Training model for up to 3594.14s of the 3594.14s of remaining time.
	0.9545	 = Validation score   (accuracy)
	0.5s	 = Training   runtime
	0.04s	 = Validation runtime
Fitting model: CatBoost ... Training model for up to 3593.59s of the 3593.59s of remaining time.
	0.9545	 = Validation score   (accuracy)
	1.72s	 = Training   runtime
	0.0s	 = Validation runtime
Fitting model: ExtraTreesGini ... Training model for up to 3591.87s of the 3591.87s of remaining time.
	0.9545	 = Validation score   (accuracy)
	0.47s	 = Training   runtime
	0.04s	 = Validation runtime
Fitting model: ExtraTreesEntr ... Training model for up to 3591.35s of the 3591.35s of remaining time.
	0.9545	 = Validation score   (accuracy)
	0.46s	 = Training   runtime
	0.04s	 = Validation runtime
Fitting model: XGBoost ... Training model for up to 3590.84s of the 3590.83s of remaining time.
	0.9091	 = Validation score   (accuracy)
	0.25s	 = Training   runtime
	0.02s	 = Validation runtime
Fitting model: NeuralNetTorch ... Training model for up to 3590.56s of the 3590.56s of remaining time.
	0.9545	 = Validation score   (accuracy)
	2.74s	 = Training   runtime
	0.01s	 = Validation runtime
Fitting model: LightGBMLarge ... Training model for up to 3587.79s of the 3587.79s of remaining time.
	0.8636	 = Validation score   (accuracy)
	0.42s	 = Training   runtime
	0.0s	 = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3587.33s of remaining time.
	Ensemble Weights: {'ExtraTreesEntr': 1.0}
	0.9545	 = Validation score   (accuracy)
	0.11s	 = Training   runtime
	0.0s	 = Validation runtime
AutoGluon training complete, total runtime = 12.81s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 568.7 rows/s (22 batch size)
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("models\Teacher\medium_quality-20241120-233321")
