{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查找最小数据集   \n",
    "### version 1.0   \n",
    "利用已有的分类数据分别计算各类别的质心，然后计算样本到质心的距离，删除特定距离的样本，查找最小数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dataset_threshold = 0.80        # minimum minimum dataset threshold for the dataset\n",
    "min_dataset_method = 'proportion'         # minimum dataset method:        'first' , 'last' or 'proportion'\n",
    "taxonomy = 'Berg'                   # choose which taxonomy:        'Berg', 'SEA_AD' or 'Mouse'\n",
    "cell_subset = 'Superficial'         # choose which cells:           'All', 'Superficial' or 'Deep'\n",
    "modality = 'Ephys'                  # choose which modality:        'Both', 'Ephys' or 'Morph'\n",
    "rowfilte = True                     # choose if you want to row filter: True or False\n",
    "scaler_method = None                # choose your data format:      'standard','minmax','robust','normalizer','quantile'\n",
    "impute_method = 'knn'                # choose which imputer:         None or 'knn' or 'liner' or 'Polynomial' or 'decisiontree':\n",
    "oversample_method = 'random'            # choose which oversampler:      None or 'random' or 'smote' or 'smoten' or 'smotenc' or 'borderline' or 'adasyn' or 'svmsmote' or 'kmeans'\n",
    "# 设置缺省参数\n",
    "min_cells_per_type = 5              # minimum cells necessary in the training data per group/label/cell-type (e.g. 5 or 10)\n",
    "null_threshold = 0.2                       # row filter null value threshold\n",
    "test_size =0.20                     # test size for cross validation (e.g. 0.2)\n",
    "teacher_model_path = 'models\\\\Teacher\\\\'\n",
    "student_model_path = 'models\\\\Student\\\\'\n",
    "train_dataset = DATA_PATH / Path(f'train_data_{cell_subset}_{modality}_{taxonomy}.csv')\n",
    "test_dataset = DATA_PATH / Path(f'test_data_{cell_subset}_{modality}_{taxonomy}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算质心\n",
    "def get_centroid(df):\n",
    "    '''\n",
    "    计算质心\n",
    "    参数:\n",
    "        df: dataframe,数据样本\n",
    "    返回值:\n",
    "        centroid: series,数据样本质心\n",
    "    '''\n",
    "    centroid = df.mean()\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(x, y, method=2):\n",
    "    '''\n",
    "    计算两个样本的距离\n",
    "\n",
    "    参数:\n",
    "      x: series,第一个样本的坐标\n",
    "      y: series,第二个样本的坐标\n",
    "      method: int,1:曼哈顿距离,2:欧氏距离\n",
    "    返回值:\n",
    "      distance: float,x到y的距离      \n",
    "    '''   \n",
    "    distance = np.power((x-y).abs().pow(method).sum(),1/method)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_dataset(df,min_dataset_threshold,method='first'):\n",
    "    \"\"\"\n",
    "    计算数据集的质心,然后计算各样本到质心的距离,按距离排序,取min_dataset_threshold%的数据\n",
    "    \n",
    "    参数:\n",
    "    df: 数据框，包含要处理的数据\n",
    "    min_dataset_threshold: 最小数据集的阈值，表示要提取的数据占总数据的百分比\n",
    "    method: 计算质心的方法，默认值为 'first'，表示取前 min_dataset_threshold% 的数据\n",
    "                                   'last' 表示取后 min_dataset_threshold% 的数据\n",
    "                                   'proportion' 表示按比例取数据，min_dataset_threshold 表示要提取的数据占总数据的比例\n",
    "\n",
    "    返回值:\n",
    "    df: 处理后的数据框\n",
    "    \"\"\"\n",
    "    # 计算数据集和最小数据集的大小\n",
    "    df_size=df.shape[0]\n",
    "    min_dataset_size = round(df_size*min_dataset_threshold)\n",
    "    # 计算数据的质心\n",
    "    centroid=get_centroid(df)\n",
    "    # 计算各样本到质心的距离\n",
    "    df['distance']=df.apply(lambda x: get_distance(x,centroid),axis=1)\n",
    "    # 按样本到质心的距离排序\n",
    "    df.sort_values(by='distance',ascending=True,inplace=True)\n",
    "    # 删除distance列\n",
    "    df.drop(columns=['distance'],inplace=True)\n",
    "    if method=='first':\n",
    "        # 取前 min_dataset_threshold% 的数据\n",
    "        end_location=min_dataset_size\n",
    "        df = df.iloc[:end_location,:]\n",
    "    elif method=='last':\n",
    "        # 取后 min_dataset_threshold% 的数据\n",
    "        start_location=df_size-min_dataset_size\n",
    "        df=df.iloc[start_location:,:]\n",
    "    elif method=='proportion':\n",
    "        # 按比例取数据\n",
    "        step=round(df_size/(df_size-min_dataset_size))\n",
    "        drop_list = df.iloc[::step].index\n",
    "        df.drop(index=drop_list,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset\n",
    "加载数据文件合并数据集,并分离标注数据和非标注数据,如果数据目录中已存在train集和test集数据则直接加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据文件：\n",
      "ephys_dataset: (522, 35)\n",
      "morph_dataset: (170, 25)\n",
      "--- Init Setup ---\n",
      "Cell_subset:  Superficial\n",
      "Modality:     Ephys\n",
      "Taxonomy:     Berg\n",
      "Labeled_data:     (230, 37)\n",
      "Unlabeled_data:   (209, 37)\n"
     ]
    }
   ],
   "source": [
    "if train_dataset.is_file() and test_dataset.is_file():\n",
    "    train_data = pd.read_csv(train_dataset, index_col=0)\n",
    "    test_data = pd.read_csv(test_dataset, index_col=0)\n",
    "    print('--- load Train & Test dataset ---')\n",
    "    print('Train_data:    ', train_data.shape)\n",
    "    print('Test_data:     ', test_data.shape)\n",
    "\n",
    "else:\n",
    "    # Define the path to the files and read in the data\n",
    "    meta_data_path = Path(\"..\\data\\meta_data_withVU.csv\")        # read the meta data file\n",
    "    ephys_path = Path(\"..\\data\\ephys_data_withVU.csv\")           # read the ephys data file\n",
    "    morph_path = Path(\"..\\data\\morph_data_withVU.csv\")           # read the morph data file\n",
    "    # Read in the data files\n",
    "    meta_data = pd.read_csv(meta_data_path, index_col=0) \n",
    "    ephys_data = pd.read_csv(ephys_path, index_col=0)\n",
    "    morph_data = pd.read_csv(morph_path, index_col=0)\n",
    "    print('原始数据文件：')\n",
    "    print('ephys_dataset:', ephys_data.shape)\n",
    "    print('morph_dataset:', morph_data.shape)\n",
    "    labeled_data, unlabeled_data =  load_data_l23_depth_normalized(meta_data, ephys_data, morph_data, taxonomy = taxonomy, modality = modality, cell_subset = cell_subset)\n",
    "    print('--- Init Setup ---')\n",
    "    print('Cell_subset: ',cell_subset)\n",
    "    print('Modality:    ', modality)\n",
    "    print('Taxonomy:    ', taxonomy)\n",
    "    print('Labeled_data:    ', labeled_data.shape)\n",
    "    print('Unlabeled_data:  ', unlabeled_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing   \n",
    "如果不存在train_data,则进行数据预处理(行过滤、补插缺失值、标准化、数据拆分未训练集和测试集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------            Dataset Row Filte            -------------------------------\n",
      "-------------------------------            Imputing Dataset            -------------------------------\n",
      "1. Imputing Labeled Dataset\n",
      "补插前标记数据集信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 230 entries, 541549258 to 811939096\n",
      "Data columns (total 37 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   input_resistance      230 non-null    float64\n",
      " 1   sag                   230 non-null    float64\n",
      " 2   VmatSag               230 non-null    float64\n",
      " 3   vmbaseM               230 non-null    float64\n",
      " 4   tau                   230 non-null    float64\n",
      " 5   FAP_rheobase          230 non-null    float64\n",
      " 6   FAP_num_APs           230 non-null    float64\n",
      " 7   TS1_rheobase          230 non-null    float64\n",
      " 8   TS1_num_APs           230 non-null    float64\n",
      " 9   TS2_rheobase          230 non-null    float64\n",
      " 10  TS2_num_APs           230 non-null    float64\n",
      " 11  TS1_adp_index         201 non-null    float64\n",
      " 12  TS2_adp_index         211 non-null    float64\n",
      " 13  FAP_threshold         230 non-null    float64\n",
      " 14  FAP_peak              230 non-null    float64\n",
      " 15  FAP_maxupstroke       230 non-null    float64\n",
      " 16  FAP_maxdownstroke     230 non-null    float64\n",
      " 17  FAP_halfwidth         230 non-null    float64\n",
      " 18  FAP_avg_upstroke      230 non-null    float64\n",
      " 19  FAP_avg_downstroke    230 non-null    float64\n",
      " 20  FAP_up_down_ratio     230 non-null    float64\n",
      " 21  TS1_threshold         230 non-null    float64\n",
      " 22  TS1_maxupstroke       230 non-null    float64\n",
      " 23  TS1_maxdownstroke     230 non-null    float64\n",
      " 24  TS1_halfwidth         230 non-null    float64\n",
      " 25  TS1_avg_upstroke      230 non-null    float64\n",
      " 26  TS1_avg_downstroke    230 non-null    float64\n",
      " 27  TS1_up_down_ratio     230 non-null    float64\n",
      " 28  TS2_threshold         230 non-null    float64\n",
      " 29  TS2_peak              230 non-null    float64\n",
      " 30  TS2_maxupstroke       230 non-null    float64\n",
      " 31  TS2_maxdownstroke     230 non-null    float64\n",
      " 32  TS2_avg_upstroke      230 non-null    float64\n",
      " 33  TS2_avg_downstroke    230 non-null    float64\n",
      " 34  TS2_up_down_ratio     230 non-null    float64\n",
      " 35  L23_depth_normalized  210 non-null    float64\n",
      " 36  label                 230 non-null    object \n",
      "dtypes: float64(36), object(1)\n",
      "memory usage: 76.4+ KB\n",
      "None\n",
      "补插后标记数据集信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230 entries, 0 to 229\n",
      "Data columns (total 37 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   input_resistance      230 non-null    float64\n",
      " 1   sag                   230 non-null    float64\n",
      " 2   VmatSag               230 non-null    float64\n",
      " 3   vmbaseM               230 non-null    float64\n",
      " 4   tau                   230 non-null    float64\n",
      " 5   FAP_rheobase          230 non-null    float64\n",
      " 6   FAP_num_APs           230 non-null    float64\n",
      " 7   TS1_rheobase          230 non-null    float64\n",
      " 8   TS1_num_APs           230 non-null    float64\n",
      " 9   TS2_rheobase          230 non-null    float64\n",
      " 10  TS2_num_APs           230 non-null    float64\n",
      " 11  TS1_adp_index         230 non-null    float64\n",
      " 12  TS2_adp_index         230 non-null    float64\n",
      " 13  FAP_threshold         230 non-null    float64\n",
      " 14  FAP_peak              230 non-null    float64\n",
      " 15  FAP_maxupstroke       230 non-null    float64\n",
      " 16  FAP_maxdownstroke     230 non-null    float64\n",
      " 17  FAP_halfwidth         230 non-null    float64\n",
      " 18  FAP_avg_upstroke      230 non-null    float64\n",
      " 19  FAP_avg_downstroke    230 non-null    float64\n",
      " 20  FAP_up_down_ratio     230 non-null    float64\n",
      " 21  TS1_threshold         230 non-null    float64\n",
      " 22  TS1_maxupstroke       230 non-null    float64\n",
      " 23  TS1_maxdownstroke     230 non-null    float64\n",
      " 24  TS1_halfwidth         230 non-null    float64\n",
      " 25  TS1_avg_upstroke      230 non-null    float64\n",
      " 26  TS1_avg_downstroke    230 non-null    float64\n",
      " 27  TS1_up_down_ratio     230 non-null    float64\n",
      " 28  TS2_threshold         230 non-null    float64\n",
      " 29  TS2_peak              230 non-null    float64\n",
      " 30  TS2_maxupstroke       230 non-null    float64\n",
      " 31  TS2_maxdownstroke     230 non-null    float64\n",
      " 32  TS2_avg_upstroke      230 non-null    float64\n",
      " 33  TS2_avg_downstroke    230 non-null    float64\n",
      " 34  TS2_up_down_ratio     230 non-null    float64\n",
      " 35  L23_depth_normalized  230 non-null    float64\n",
      " 36  label                 230 non-null    object \n",
      "dtypes: float64(36), object(1)\n",
      "memory usage: 66.6+ KB\n",
      "None\n",
      "2. Imputing Unlabeled Dataset\n",
      "补插前未标记数据集信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 209 entries, 110000001 to H23.29.252.11.91.02\n",
      "Data columns (total 37 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   input_resistance      207 non-null    float64\n",
      " 1   sag                   207 non-null    float64\n",
      " 2   VmatSag               207 non-null    float64\n",
      " 3   vmbaseM               209 non-null    float64\n",
      " 4   tau                   200 non-null    float64\n",
      " 5   FAP_rheobase          209 non-null    float64\n",
      " 6   FAP_num_APs           209 non-null    float64\n",
      " 7   TS1_rheobase          209 non-null    float64\n",
      " 8   TS1_num_APs           208 non-null    float64\n",
      " 9   TS2_rheobase          208 non-null    float64\n",
      " 10  TS2_num_APs           208 non-null    float64\n",
      " 11  TS1_adp_index         186 non-null    float64\n",
      " 12  TS2_adp_index         196 non-null    float64\n",
      " 13  FAP_threshold         209 non-null    float64\n",
      " 14  FAP_peak              187 non-null    float64\n",
      " 15  FAP_maxupstroke       209 non-null    float64\n",
      " 16  FAP_maxdownstroke     209 non-null    float64\n",
      " 17  FAP_halfwidth         209 non-null    float64\n",
      " 18  FAP_avg_upstroke      209 non-null    float64\n",
      " 19  FAP_avg_downstroke    209 non-null    float64\n",
      " 20  FAP_up_down_ratio     209 non-null    float64\n",
      " 21  TS1_threshold         209 non-null    float64\n",
      " 22  TS1_maxupstroke       209 non-null    float64\n",
      " 23  TS1_maxdownstroke     209 non-null    float64\n",
      " 24  TS1_halfwidth         209 non-null    float64\n",
      " 25  TS1_avg_upstroke      209 non-null    float64\n",
      " 26  TS1_avg_downstroke    209 non-null    float64\n",
      " 27  TS1_up_down_ratio     209 non-null    float64\n",
      " 28  TS2_threshold         208 non-null    float64\n",
      " 29  TS2_peak              208 non-null    float64\n",
      " 30  TS2_maxupstroke       208 non-null    float64\n",
      " 31  TS2_maxdownstroke     208 non-null    float64\n",
      " 32  TS2_avg_upstroke      208 non-null    float64\n",
      " 33  TS2_avg_downstroke    208 non-null    float64\n",
      " 34  TS2_up_down_ratio     208 non-null    float64\n",
      " 35  L23_depth_normalized  209 non-null    float64\n",
      " 36  label                 0 non-null      object \n",
      "dtypes: float64(36), object(1)\n",
      "memory usage: 62.0+ KB\n",
      "None\n",
      "补插后未标记数据集信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209 entries, 0 to 208\n",
      "Data columns (total 37 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   input_resistance      209 non-null    float64\n",
      " 1   sag                   209 non-null    float64\n",
      " 2   VmatSag               209 non-null    float64\n",
      " 3   vmbaseM               209 non-null    float64\n",
      " 4   tau                   209 non-null    float64\n",
      " 5   FAP_rheobase          209 non-null    float64\n",
      " 6   FAP_num_APs           209 non-null    float64\n",
      " 7   TS1_rheobase          209 non-null    float64\n",
      " 8   TS1_num_APs           209 non-null    float64\n",
      " 9   TS2_rheobase          209 non-null    float64\n",
      " 10  TS2_num_APs           209 non-null    float64\n",
      " 11  TS1_adp_index         209 non-null    float64\n",
      " 12  TS2_adp_index         209 non-null    float64\n",
      " 13  FAP_threshold         209 non-null    float64\n",
      " 14  FAP_peak              209 non-null    float64\n",
      " 15  FAP_maxupstroke       209 non-null    float64\n",
      " 16  FAP_maxdownstroke     209 non-null    float64\n",
      " 17  FAP_halfwidth         209 non-null    float64\n",
      " 18  FAP_avg_upstroke      209 non-null    float64\n",
      " 19  FAP_avg_downstroke    209 non-null    float64\n",
      " 20  FAP_up_down_ratio     209 non-null    float64\n",
      " 21  TS1_threshold         209 non-null    float64\n",
      " 22  TS1_maxupstroke       209 non-null    float64\n",
      " 23  TS1_maxdownstroke     209 non-null    float64\n",
      " 24  TS1_halfwidth         209 non-null    float64\n",
      " 25  TS1_avg_upstroke      209 non-null    float64\n",
      " 26  TS1_avg_downstroke    209 non-null    float64\n",
      " 27  TS1_up_down_ratio     209 non-null    float64\n",
      " 28  TS2_threshold         209 non-null    float64\n",
      " 29  TS2_peak              209 non-null    float64\n",
      " 30  TS2_maxupstroke       209 non-null    float64\n",
      " 31  TS2_maxdownstroke     209 non-null    float64\n",
      " 32  TS2_avg_upstroke      209 non-null    float64\n",
      " 33  TS2_avg_downstroke    209 non-null    float64\n",
      " 34  TS2_up_down_ratio     209 non-null    float64\n",
      " 35  L23_depth_normalized  209 non-null    float64\n",
      " 36  label                 0 non-null      float64\n",
      "dtypes: float64(37)\n",
      "memory usage: 60.5 KB\n",
      "None\n",
      "dataset rownum: 230\n",
      "feature num: 36\n",
      "feature list: ['input_resistance' 'sag' 'VmatSag' 'vmbaseM' 'tau' 'FAP_rheobase'\n",
      " 'FAP_num_APs' 'TS1_rheobase' 'TS1_num_APs' 'TS2_rheobase' 'TS2_num_APs'\n",
      " 'TS1_adp_index' 'TS2_adp_index' 'FAP_threshold' 'FAP_peak'\n",
      " 'FAP_maxupstroke' 'FAP_maxdownstroke' 'FAP_halfwidth' 'FAP_avg_upstroke'\n",
      " 'FAP_avg_downstroke' 'FAP_up_down_ratio' 'TS1_threshold'\n",
      " 'TS1_maxupstroke' 'TS1_maxdownstroke' 'TS1_halfwidth' 'TS1_avg_upstroke'\n",
      " 'TS1_avg_downstroke' 'TS1_up_down_ratio' 'TS2_threshold' 'TS2_peak'\n",
      " 'TS2_maxupstroke' 'TS2_maxdownstroke' 'TS2_avg_upstroke'\n",
      " 'TS2_avg_downstroke' 'TS2_up_down_ratio' 'L23_depth_normalized']\n",
      "cell type num: 3\n",
      "cell type list: ['Exc L2 LAMP5 LTK' 'Exc L2-3 LINC00507 FREM3 superficial'\n",
      " 'Exc L2-4 LINC00507 GLP2R']\n",
      "-------------------------------    Train & Test Dataset Split    -------------------------------\n",
      "train: (184, 37)\n",
      "test: (46, 37)\n"
     ]
    }
   ],
   "source": [
    "if 'train_data' not in locals():\n",
    "\n",
    "    if rowfilte:\n",
    "        print('-------------------------------            Dataset Row Filte            -------------------------------')    \n",
    "        labeled_data = filter(labeled_data,min_cells_per_type=min_cells_per_type, threshold=null_threshold)\n",
    "\n",
    "    if impute_method:\n",
    "        print('-------------------------------            Imputing Dataset            -------------------------------')\n",
    "        print('1. Imputing Labeled Dataset')\n",
    "        # 不包含标签列进行补插\n",
    "        print('补插前标记数据集信息:')\n",
    "        t_type_labels = labeled_data.iloc[:, -1].values\n",
    "        print(labeled_data.info())\n",
    "        labeled_data = impute(labeled_data.iloc[:,0:labeled_data.columns.size-1],method=impute_method)\n",
    "        labeled_data[LABEL_COLUMN] = t_type_labels\n",
    "        print('补插后标记数据集信息:')\n",
    "        print(labeled_data.info())\n",
    "        print('2. Imputing Unlabeled Dataset')\n",
    "        # 对unlabel_data数据集补插 \n",
    "        print('补插前未标记数据集信息:')\n",
    "        print(unlabeled_data.info())\n",
    "        unlabeled_data = impute(unlabeled_data.iloc[:,0:unlabeled_data.columns.size-1],method=impute_method)\n",
    "        unlabeled_data[LABEL_COLUMN] = np.nan\n",
    "        print('补插后未标记数据集信息:')\n",
    "        print(unlabeled_data.info())\n",
    "\n",
    "    # 转换为numpy数组\n",
    "    data_array = labeled_data.iloc[:,0:labeled_data.columns.size-1].values              # transform the data to an array\n",
    "    # Check if data needs to be scoring\n",
    "    if scaler_method:\n",
    "        print('-------------------------------             Scaling Dataset            -------------------------------')\n",
    "        data_array = scale(data_array, scalemethod = scaler_method)                     # apply scoring method \n",
    "    cell_ids_subset = labeled_data.index                                                # extract the cell IDs of the subset\n",
    "    # Create a list of features\n",
    "    feature_list = labeled_data.iloc[:,0:labeled_data.columns.size-1].keys().values\n",
    "    # normalized_depths = labeled_data['L23_depth_normalized'].values                     # extract normalized depth values\n",
    "    t_type_labels = labeled_data.iloc[:, -1].values                                     # extract t-type labels of the data subset\n",
    "    t_types_updated = np.unique(t_type_labels)\n",
    "    print('dataset rownum:',data_array.shape[0])\n",
    "    print('feature num:',len(feature_list))\n",
    "    print('feature list:',feature_list)\n",
    "    print('cell type num:',len(t_types_updated))\n",
    "    print('cell type list:',t_types_updated)\n",
    "    # Split the data into a training and test dataset\n",
    "    print('-------------------------------    Train & Test Dataset Split    -------------------------------')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_array, t_type_labels, test_size=test_size, stratify=t_type_labels, random_state=RANDOM_STATE)\n",
    "\n",
    "    # 组合X_train数据为dataframe，未包含label列\n",
    "    train_data= pd.DataFrame(data = X_train, columns = feature_list)\n",
    "    train_data[LABEL_COLUMN] = y_train\n",
    "    print('train:',train_data.shape)\n",
    "\n",
    "    # 测试集\n",
    "    test_data_nolabel = pd.DataFrame(data = X_test, columns = feature_list)\n",
    "    test_data = test_data_nolabel.copy()\n",
    "    test_data[LABEL_COLUMN] = pd.DataFrame(y_test, columns = [LABEL_COLUMN])\n",
    "    print('test:',test_data.shape)\n",
    "\n",
    "    # 保存训练集和测试集\n",
    "    train_data.to_csv(f'..\\data\\\\train_data_{cell_subset}_{modality}_{taxonomy}.csv')\n",
    "    test_data.to_csv(f'..\\data\\\\test_data_{cell_subset}_{modality}_{taxonomy}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reduce Dataset   \n",
    "按类计算各类的质心和每个样本到质心的距离，按照从小到大的顺序排列，按min_dataset_threshold比例过滤每个类样本，并合并未新的训练集。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process group:('Exc L2 LAMP5 LTK',):61\n",
      "process group:('Exc L2-3 LINC00507 FREM3 superficial',):103\n",
      "process group:('Exc L2-4 LINC00507 GLP2R',):20\n",
      "label\n",
      "Exc L2-3 LINC00507 FREM3 superficial    82\n",
      "Exc L2 LAMP5 LTK                        48\n",
      "Exc L2-4 LINC00507 GLP2R                16\n",
      "Name: count, dtype: int64\n",
      "new_train_data: (146, 37)\n"
     ]
    }
   ],
   "source": [
    "new_train_data=None\n",
    "for group in train_data.groupby(by=[LABEL_COLUMN]):\n",
    "    df=group[1].iloc[:,0:group[1].columns.size-1]\n",
    "    print(f'process group:{group[0]}:{df.shape[0]}')\n",
    "    df=get_min_dataset(df,min_dataset_threshold,method=min_dataset_method)\n",
    "    df[LABEL_COLUMN]=group[1].iloc[:,-1]\n",
    "    if new_train_data is None:\n",
    "        new_train_data=df\n",
    "    else:\n",
    "        new_train_data=pd.concat([new_train_data,df])\n",
    "\n",
    "print(new_train_data[LABEL_COLUMN].value_counts())\n",
    "print('new_train_data:',new_train_data.shape)\n",
    "train_data = new_train_data\n",
    "# 按索引排序,否则训练结果会与原数据集训练结果不一致\n",
    "train_data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. oversample\n",
    "过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------           Dataset Oversample          -------------------------------\n",
      "oversample train: (246, 37)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data.iloc[:,-1]\n",
    "train_data = train_data.iloc[:,:train_data.columns.size-1]\n",
    "feature_list = train_data.columns.tolist()\n",
    "if oversample_method:\n",
    "    print('-------------------------------           Dataset Oversample          -------------------------------')\n",
    "    # 过采样的数据集不能有空值\n",
    "    # 当程序未设置插值方法,这里使用默认的knn方法,否则使用设定的插值方法\n",
    "    if impute_method:\n",
    "        train_data = impute(train_data, method = impute_method, random_state = RANDOM_STATE)\n",
    "    else:\n",
    "        train_data = impute(train_data, random_state = RANDOM_STATE)\n",
    "    # 进行过采样\n",
    "    X_train, y_train = oversampler(train_data.values, y_train, method = oversample_method)    \n",
    "    train_data= pd.DataFrame(data = X_train, columns = feature_list)\n",
    "train_data[LABEL_COLUMN] = y_train\n",
    "print('oversample train:',train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. train model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.11.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          8\n",
      "Memory Avail:       9.16 GB / 15.92 GB (57.5%)\n",
      "Disk Space Avail:   307.31 GB / 931.51 GB (33.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"models\\Teacher\\medium_quality-20241106-204955\"\n",
      "Train Data Rows:    246\n",
      "Train Data Columns: 36\n",
      "Label Column:       label\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\t3 unique label values:  ['Exc L2-3 LINC00507 FREM3 superficial', 'Exc L2 LAMP5 LTK', 'Exc L2-4 LINC00507 GLP2R']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9373.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 36 | ['input_resistance', 'sag', 'VmatSag', 'vmbaseM', 'tau', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 36 | ['input_resistance', 'sag', 'VmatSag', 'vmbaseM', 'tau', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t36 features in original data used to generate 36 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 196, Val Rows: 50\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.88s of the 3599.87s of remaining time.\n",
      "\t0.68\t = Validation score   (accuracy)\n",
      "\t2.51s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3597.15s of the 3597.15s of remaining time.\n",
      "\t0.78\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3597.13s of the 3597.13s of remaining time.\n",
      "\t0.86\t = Validation score   (accuracy)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3594.55s of the 3594.54s of remaining time.\n",
      "\t0.82\t = Validation score   (accuracy)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3594.08s of the 3594.08s of remaining time.\n",
      "\t0.84\t = Validation score   (accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3593.67s of the 3593.66s of remaining time.\n",
      "\t0.82\t = Validation score   (accuracy)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3592.55s of the 3592.54s of remaining time.\n",
      "\t0.82\t = Validation score   (accuracy)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3591.61s of the 3591.6s of remaining time.\n",
      "\t0.82\t = Validation score   (accuracy)\n",
      "\t3.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3588.36s of the 3588.35s of remaining time.\n",
      "\t0.84\t = Validation score   (accuracy)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3587.25s of the 3587.24s of remaining time.\n",
      "\t0.84\t = Validation score   (accuracy)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3586.3s of the 3586.29s of remaining time.\n",
      "\t0.84\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3585.74s of the 3585.73s of remaining time.\n",
      "\t0.9\t = Validation score   (accuracy)\n",
      "\t10.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3575.37s of the 3575.37s of remaining time.\n",
      "\t0.84\t = Validation score   (accuracy)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 359.99s of the 3573.1s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.9\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 27.16s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3743.3 rows/s (50 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"models\\Teacher\\medium_quality-20241106-204955\")\n"
     ]
    }
   ],
   "source": [
    "#设置训练精度\n",
    "presets='medium_quality'\n",
    "eval_metric = 'accuracy'\n",
    "verbosity = 2\n",
    "time_limit = 3600\n",
    "auto_stack=False\n",
    "# bagging的折数,缺省好像未8\n",
    "num_bag_folds=5\n",
    "# stacking的级别\n",
    "num_stack_levels=2\n",
    "num_bag_sets=1\n",
    "# n折交叉验证,设置auto_stack为True和dynamic_stacking为True\n",
    "dynamic_stacking = False\n",
    "# n_fold为交叉验证折数,n_repeats为交叉验证重复次数\n",
    "ds_args = {\n",
    "    'n_folds': 5,\n",
    "    'n_repeats': 1,\n",
    "}\n",
    "# 模型的保存路径，详细的模型命名规则见：https://auto.gluon.ai/stable/api/autogluon.tabular.models.html\n",
    "save_path = teacher_model_path + presets + datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 开始训练\n",
    "predictor = TabularPredictor(label=LABEL_COLUMN, verbosity=verbosity,eval_metric=eval_metric, path=save_path, log_to_file=True,log_file_path='auto')\n",
    "if auto_stack and dynamic_stacking:\n",
    "    predictor.fit(train_data, presets=presets, time_limit=time_limit, auto_stack=auto_stack, num_bag_folds=num_bag_folds, \n",
    "                        num_stack_levels=num_stack_levels, num_bag_sets=num_bag_sets,dynamic_stacking=dynamic_stacking, ds_args=ds_args)\n",
    "elif auto_stack and not dynamic_stacking:\n",
    "    predictor.fit(train_data, presets=presets, time_limit=time_limit, auto_stack=auto_stack, num_bag_folds=num_bag_folds,\n",
    "                        num_stack_levels=num_stack_levels, num_bag_sets=num_bag_sets)\n",
    "elif not auto_stack and dynamic_stacking:\n",
    "    predictor.fit(train_data, presets=presets, time_limit=time_limit, auto_stack=auto_stack, dynamic_stacking=dynamic_stacking, ds_args=ds_args)\n",
    "else:\n",
    "    predictor.fit(train_data, presets=presets, time_limit=time_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon infers problem type is:  multiclass\n",
      "AutoGluon identified the following types of features:\n",
      "('float', []) : 36 | ['input_resistance', 'sag', 'VmatSag', 'vmbaseM', 'tau', ...]\n",
      "Best Model: WeightedEnsemble_L2\n",
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2       0.90    accuracy       0.013357  10.543807                0.000000           0.219320            2       True         14\n",
      "1        NeuralNetTorch       0.90    accuracy       0.013357  10.324487                0.013357          10.324487            1       True         12\n",
      "2       NeuralNetFastAI       0.86    accuracy       0.013275   2.551274                0.013275           2.551274            1       True          3\n",
      "3              LightGBM       0.84    accuracy       0.000000   0.397178                0.000000           0.397178            1       True          5\n",
      "4         LightGBMLarge       0.84    accuracy       0.002506   2.091611                0.002506           2.091611            1       True         13\n",
      "5               XGBoost       0.84    accuracy       0.003509   0.536651                0.003509           0.536651            1       True         11\n",
      "6        ExtraTreesEntr       0.84    accuracy       0.069103   0.862696                0.069103           0.862696            1       True         10\n",
      "7        ExtraTreesGini       0.84    accuracy       0.091687   0.985145                0.091687           0.985145            1       True          9\n",
      "8            LightGBMXT       0.82    accuracy       0.000000   0.453762                0.000000           0.453762            1       True          4\n",
      "9              CatBoost       0.82    accuracy       0.000000   3.250328                0.000000           3.250328            1       True          8\n",
      "10     RandomForestEntr       0.82    accuracy       0.057922   0.866408                0.057922           0.866408            1       True          7\n",
      "11     RandomForestGini       0.82    accuracy       0.101894   0.950910                0.101894           0.950910            1       True          6\n",
      "12       KNeighborsDist       0.78    accuracy       0.010175   0.010133                0.010175           0.010133            1       True          2\n",
      "13       KNeighborsUnif       0.68    accuracy       0.207273   2.506818                0.207273           2.506818            1       True          1\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'LGBModel', 'XTModel', 'CatBoostModel', 'KNNModel', 'XGBoostModel', 'RFModel', 'NNFastAiTabularModel', 'WeightedEnsembleModel', 'TabularNeuralNetTorchModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', []) : 36 | ['input_resistance', 'sag', 'VmatSag', 'vmbaseM', 'tau', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonprj\\env\\tcell-QkBhXdaq-py3.11\\Lib\\site-packages\\autogluon\\core\\utils\\plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)\n",
    "print(\"Best Model:\", predictor.model_best)\n",
    "result = predictor.fit_summary(show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedEnsemble_L2\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                    Exc L2 LAMP5 LTK       0.44      0.47      0.45        15\n",
      "Exc L2-3 LINC00507 FREM3 superficial       0.69      0.69      0.69        26\n",
      "            Exc L2-4 LINC00507 GLP2R       0.75      0.60      0.67         5\n",
      "\n",
      "                            accuracy                           0.61        46\n",
      "                           macro avg       0.63      0.59      0.60        46\n",
      "                        weighted avg       0.62      0.61      0.61        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = predictor.predict(test_data.iloc[:,:test_data.columns.size-1])#,model=predictor.model_best)\n",
    "print(predictor.model_best)\n",
    "results = pd.concat([y_pred, test_data.iloc[:,-1]], axis=1)\n",
    "results.columns=['predicted', 'actual']\n",
    "# 打印每个类的精确度，召回率，F1值, 由于样本量较少,会出现被0除, 计算结果为0，但会出现警告错误：\n",
    "# UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. \n",
    "# Use `zero_division` parameter to control this behavior._warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "print(classification_report(results['actual'], results['predicted'], target_names=np.unique(test_data.iloc[:,-1])))\n",
    "# 输出预测值与实际值对比\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0            LightGBMXT    0.739130       0.82    accuracy        0.005002   \n",
      "1              LightGBM    0.717391       0.84    accuracy        0.002997   \n",
      "2       NeuralNetFastAI    0.673913       0.86    accuracy        0.025736   \n",
      "3         LightGBMLarge    0.673913       0.84    accuracy        0.033640   \n",
      "4      RandomForestGini    0.673913       0.82    accuracy        0.094247   \n",
      "5        ExtraTreesEntr    0.673913       0.84    accuracy        0.102520   \n",
      "6      RandomForestEntr    0.673913       0.82    accuracy        0.105177   \n",
      "7              CatBoost    0.652174       0.82    accuracy        0.007412   \n",
      "8        ExtraTreesGini    0.630435       0.84    accuracy        0.112040   \n",
      "9   WeightedEnsemble_L2    0.608696       0.90    accuracy        0.025383   \n",
      "10       NeuralNetTorch    0.608696       0.90    accuracy        0.025383   \n",
      "11              XGBoost    0.543478       0.84    accuracy        0.025025   \n",
      "12       KNeighborsDist    0.369565       0.78    accuracy        0.004752   \n",
      "13       KNeighborsUnif    0.347826       0.68    accuracy        0.013100   \n",
      "\n",
      "    pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
      "0        0.000000   0.453762                 0.005002                0.000000   \n",
      "1        0.000000   0.397178                 0.002997                0.000000   \n",
      "2        0.013275   2.551274                 0.025736                0.013275   \n",
      "3        0.002506   2.091611                 0.033640                0.002506   \n",
      "4        0.101894   0.950910                 0.094247                0.101894   \n",
      "5        0.069103   0.862696                 0.102520                0.069103   \n",
      "6        0.057922   0.866408                 0.105177                0.057922   \n",
      "7        0.000000   3.250328                 0.007412                0.000000   \n",
      "8        0.091687   0.985145                 0.112040                0.091687   \n",
      "9        0.013357  10.543807                 0.000000                0.000000   \n",
      "10       0.013357  10.324487                 0.025383                0.013357   \n",
      "11       0.003509   0.536651                 0.025025                0.003509   \n",
      "12       0.010175   0.010133                 0.004752                0.010175   \n",
      "13       0.207273   2.506818                 0.013100                0.207273   \n",
      "\n",
      "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
      "0            0.453762            1       True          4  \n",
      "1            0.397178            1       True          5  \n",
      "2            2.551274            1       True          3  \n",
      "3            2.091611            1       True         13  \n",
      "4            0.950910            1       True          6  \n",
      "5            0.862696            1       True         10  \n",
      "6            0.866408            1       True          7  \n",
      "7            3.250328            1       True          8  \n",
      "8            0.985145            1       True          9  \n",
      "9            0.219320            2       True         14  \n",
      "10          10.324487            1       True         12  \n",
      "11           0.536651            1       True         11  \n",
      "12           0.010133            1       True          2  \n",
      "13           2.506818            1       True          1  \n"
     ]
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(test_data,extra_info=False, silent=True)\n",
    "print(leaderboard)\n",
    "pst = presets.split('_')[0]\n",
    "cv = ''\n",
    "if auto_stack and dynamic_stacking:\n",
    "    validation_procedure=ds_args['validation_procedure']\n",
    "    folds=ds_args['n_folds']\n",
    "    repeats=ds_args['n_repeats']\n",
    "    cv =f\"-{validation_procedure}{folds}{repeats}\"\n",
    "leaderboard.to_csv(f'mindataset-{min_dataset_method}-{int(min_dataset_threshold*100)}-{modality}-{cell_subset}-{pst}{cv}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcell-QkBhXdaq-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
